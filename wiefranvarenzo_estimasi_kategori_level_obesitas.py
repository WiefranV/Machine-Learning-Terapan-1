# -*- coding: utf-8 -*-
"""WiefranVarenzo_Estimasi_Kategori_Level_Obesitas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tP0C9KfF8FWf2yvfl-ZVXl049-pEVcKN

#**Estimasi Kategori Level Obesitas Berdasarkan Kondisi fisik dan Pola Makan**

*   Nama    : Wiefran Varenzo
*   Email   : lionwiefran88@gmail.com
*   Username: Wiefran Varenzo

Proyek ini bertujuan untuk memprediksi tingkat obesitas seseorang berdasarkan kebiasaan makan dan kondisi fisik pengguna. Dengan memanfaatkan algoritma pembelajaran mesin, sistem ini dibangun untuk membantu memahami faktor-faktor yang berkontribusi terhadap obesitas dan memberikan prediksi berdasarkan kondisi fisik yang dikumpulkan melalui data survei.

# **1. Perkenalan Dataset**

## **Sumber Dataset**

Dataset ini diambil dari [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition) dengan judul:
**"Estimation of Obesity Levels Based On Eating Habits and Physical Condition"**

## **Gambaran Umum Dataset**

* **Jumlah Baris**: 2.111
* **Jumlah Kolom**: 17 fitur yang mencerminkan kebiasaan makan dan gaya hidup
* **Jenis Data**: Kombinasi data **kategorikal**, **biner**, dan **numerikal**

### **Tipe Fitur**

* **Numerikal (Kontinu & Integer)**:

  * `Age` (Umur)
  * `Height` (Tinggi badan)
  * `Weight` (Berat badan)
  * `FCVC` (Frekuensi makan sayur)
  * `NCP` (Jumlah makan utama per hari)
  * `CH2O` (Jumlah konsumsi air per hari)
  * `FAF` (Frekuensi aktivitas fisik)
  * `TUE` (Waktu penggunaan perangkat teknologi)

* **Kategorikal / Biner**:

  * `Gender` (Jenis kelamin)
  * `family_history_with_overweight` (Riwayat keluarga dengan obesitas)
  * `FAVC` (Konsumsi makanan tinggi kalori)
  * `CAEC` (Makan di antara jam makan utama)
  * `SMOKE` (Merokok atau tidak)
  * `SCC` (Pemantauan kalori harian)
  * `CALC` (Frekuensi konsumsi alkohol)
  * `MTRANS` (Jenis transportasi yang digunakan)

* **Target**:

  * `NObeyesdad`: Level obesitas yang diprediksi (kategori klasifikasi)

# **2. Import Library**

Impor library ini mencakup seluruh pipeline machine learning: mulai dari manipulasi data (pandas, numpy), visualisasi (matplotlib, seaborn), preprocessing dan feature selection (scikit-learn, SMOTE), hingga modeling (Random Forest, SVM) dan evaluasi (akurasi, F1-score, dll). Juga disertakan tools untuk download dataset (requests, zipfile) dan penyimpanan model (joblib). Pendeknya, ini adalah toolkit lengkap untuk proyek klasifikasi obesitas berbasis kondisi fisik dan pola hidup.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import requests
import zipfile
import io
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import RFE, mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, cohen_kappa_score,
    matthews_corrcoef
)
from imblearn.over_sampling import SMOTE
from scipy.stats import skew

"""# **3. Memuat Dataset**

Cell ini secara otomatis mengunduh dan mengekstrak dataset obesitas dari UCI Repository, lalu membaca file CSV di dalamnya ke dalam DataFrame df menggunakan pandas.
"""

url = "https://archive.ics.uci.edu/static/public/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition.zip"

response = requests.get(url)
with zipfile.ZipFile(io.BytesIO(response.content)) as z:
    print("Files in ZIP:", z.namelist())

    with z.open("ObesityDataSet_raw_and_data_sinthetic.csv") as csv_file:
        df = pd.read_csv(csv_file)

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Saya melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset. EDA disini bertujuan untuk:

1. **Memahami Struktur Data**
   - Saya menggunakan df.info() untuk melihat informasi umum mengenai dataset seperti jumlah baris, jumlah kolom, dan tipe data di masing-masing kolom.
   - df.describe() digunakan untuk menghasilkan statistik deskriptif dari fitur numerik seperti rata-rata (mean), standar deviasi (std), nilai minimum, maksimum, serta kuartil (25%, 50%, dan 75%).

2. **Menangani Data yang Hilang**  
   - Saya menggunakan df.info() untuk melihat apakah terdapat nilai yang hilang (null/missing) pada kolom apa pun.
   - Berdasarkan hasilnya, tidak ada nilai yang hilang secara eksplisit (Non-Null Count dari semua kolom menunjukkan tidak ada kekurangan data).

3. **Analisis Distribusi dan Korelasi**  
   - Distribusi kelas target NObeyesdad dianalisis menggunakan:
    casa

    *   df.Gender.value_counts()
    *   df.CAEC.value_counts()
    *   df.CALC.value_counts()
    *   df.MTRANS.value_counts()


4. **Visualisasi Data**  
   - Histogram dan boxplot: untuk kolom numerik (Age, Height, Weight, dsb).
   - Diagram batang (bar plot): untuk kolom kategorikal (Gender, CAEC, CALC, dsb).
   - Heatmap korelasi: untuk melihat hubungan antar fitur numerik.
   - Pairplot: untuk kombinasi visual antar beberapa fitur sekaligus.

## Memahami Struktur Data
"""

df

"""Disini saya ingin melihat apa saja data yang terdapat di dataset, sehingga saya memanggil df, untuk menampilkan dataframenya."""

# Melihat persebaran dan distribusi data
df.describe()

"""Describe() digunakan untuk mengetahui persebaran data, baik itu Q1, Q2, Q3, mean, min, max, dan standar deviasi dari datset numerik

"""

# Melihat informasi tipe data dan jumlah data tiap kolom
df.info()

"""Untuk setiap kolom, tipe datanya sudah tepat, kecuali TransactionDate  dan PreviousTransactionDate yang seharusnya bertipe DateTime"""

# Melihat jumlah data untuk setiap kategori Level Obesitas
df.NObeyesdad.value_counts()

"""Disini menggunakan value_count() untuk menghitung nilai unik di kolom NObeyesdad (kolom target)"""

# Data value_counts
level_obesitas = df.NObeyesdad.value_counts()
gender = df.Gender.value_counts()
caec = df.CAEC.value_counts()
calc = df.CALC.value_counts()
mtrans = df.MTRANS.value_counts()

# Buat figure dengan 2 baris dan 3 kolom
fig, axs = plt.subplots(2, 3, figsize=(18, 10))

# Flatten axs supaya mudah indexing
axs = axs.flatten()

# Plot grafik ke masing-masing subplot
axs[0].bar(level_obesitas.index, level_obesitas.values, color='skyblue')
axs[0].set_title('Level Obesitas')
axs[0].tick_params(axis='x', rotation=45)

axs[1].bar(gender.index, gender.values, color='lightgreen')
axs[1].set_title('Gender')
axs[1].tick_params(axis='x', rotation=45)

axs[2].bar(caec.index, caec.values, color='salmon')
axs[2].set_title('CAEC (Makan di antara jam utama)')
axs[2].tick_params(axis='x', rotation=45)

axs[3].bar(calc.index, calc.values, color='orange')
axs[3].set_title('CALC (Minum alkohol)')
axs[3].tick_params(axis='x', rotation=45)

axs[4].bar(mtrans.index, mtrans.values, color='purple')
axs[4].set_title('MTRANS (Tipe Transportasi)')
axs[4].tick_params(axis='x', rotation=45)

# Kosongkan subplot ke-6 yang gak dipakai
axs[5].axis('off')

plt.tight_layout()
plt.show()

"""Bisa dilihat bahwa dataset ini adalah hasil responden data yang berimbang untuk gender laki-laki dan perempuan, sedangkan memiliki keragaman yang cukup tinggi di kolom kategorial lainnya"""

# Melihat kolom berdasarkan tipe data Numerik atau Kategori
num_cols_df = df.select_dtypes(include=['int64', 'float64']).columns
cat_cols_df = df.select_dtypes(include=['object', 'category']).columns
print(f'Kolom Numerik: {num_cols_df}')
print(f'Kolom Kategori: {cat_cols_df}')

"""Melilhat kolom berdasarkan tipe akan sangat membantu dalam proses preprocessing, terutama dalam normalisasi feature/kolom

## Menangani Data yang Hilang dan duplikat
"""

# Melihat data null
df.isna().sum()

"""isna().sum() digunakan untuk menghitung berapa banyak data null untuk setiap kolom, dimana jika dilihat, tidak ada nilai null dalam kolom dataset."""

# Melihat seberapa banyak data duplikat
df.duplicated().sum()

"""Terdapat 24 data yang duplikat, serta tidak ada data null/missing value untuk setiap kolomnya.

## Analisis Distribusi dan Korelasi
"""

# Visualisasi distribusi label target
plt.figure(figsize=(10, 6))
sns.countplot(x='NObeyesdad', data=df, palette='Set2')
plt.title('Distribusi Kelas Target (Sebelum SMOTE)', fontsize=16)
plt.xlabel('Kategori Obesitas')
plt.ylabel('Jumlah Data')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Feature Target memiliki data yang imbalanced, sehingga diperlukan SMOTE agar tidak terjadi overfitting ke arah kelas yang memiliki data lebih banyak."""

# Menghitung korelasi untuk setiap kolom numerik
valid_num_cols = [col for col in num_cols_df if col in df.columns]
corr_matrix = df[valid_num_cols].corr()
print(corr_matrix)

"""Disini, perhitungan tingkat korelasi adalah dasar dari pembuatan heatmap korelasi untuk mengetahui dampak suatu feature dengan feature lainnya."""

# Visualisasi korelasi untuk setiap kolom numerik
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap Korelasi Kolom Numerik df2 (Presisi Tinggi)')
plt.show()

"""Kalau dilihat, weight dan height cukup berkorelasi, dan juga FAF (aktivitas fisik) serta height juga berkorelasi cukup baik juga.

## Visualisasi Data
"""

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(18, 16))
fig.suptitle('Visualisasi Dataset Obesitas', fontsize=18)

# 1. Top Transportasi yang Dipakai (Top 5)
top_transport = df['MTRANS'].value_counts().nlargest(5).index
sns.countplot(data=df[df['MTRANS'].isin(top_transport)], x='MTRANS', hue='MTRANS', order=top_transport, palette='viridis', legend=False, ax=axes[0, 0])
axes[0, 0].set_title('Top 5 Mode Transportasi')
axes[0, 0].tick_params(axis='x', rotation=45)

# 2. Rata-rata Berat berdasarkan Level Obesitas
sns.barplot(data=df, x='NObeyesdad', y='Weight', hue='NObeyesdad', estimator='mean', palette='coolwarm', legend=False, ax=axes[0, 1])
axes[0, 1].set_title('Rata-rata Berat per Level Obesitas')
axes[0, 1].tick_params(axis='x', rotation=45)

# 3. Distribusi Usia
sns.histplot(df['Age'], bins=20, kde=True, color='skyblue', legend=False, ax=axes[1, 0])
axes[1, 0].set_title('Distribusi Usia')

# 4. Aktivitas Fisik vs Berat Badan
sns.scatterplot(data=df, x='FAF', y='Weight', hue='Gender', alpha=0.7, legend=False, ax=axes[1, 1])
axes[1, 1].set_title('Aktivitas Fisik vs Berat Badan')

# 5. Konsumsi Sayuran Harian vs Obesitas (Distribusi)
sns.countplot(data=df, x='FAVC', hue='NObeyesdad', palette='Set2', legend=False, ax=axes[2, 0])
axes[2, 0].set_title('Konsumsi Sayuran Harian vs Obesitas')

# 6. Boxplot Berat per Gender
sns.boxplot(data=df, x='Gender', y='Weight', hue='Gender', palette='Set3', legend=False, ax=axes[2, 1])
axes[2, 1].set_title('Distribusi Berat Badan per Gender')

plt.tight_layout(rect=[0, 0, 1, 0.97])  # space for main title
plt.show()

"""Disini saya bisa kita lihat bahwa distribusi nilai numerik sangat beragam. Berikut beberapa insight yang bisa kita temukan:

* lebih dari setengah responden memilih public transportation sebegai alat mobilitas
* Rata-rata berat obesitas berdasarkan level memiliki perbedaan yang cukup significan, tetapi untuk obesitas level II dan Obesitas level III, perbedaan rata-rata berat badannya cukup kecil
* Weight di atas 120 tidak ada yang memiliki frekuensi berolahraga lebih dari 2 kali.

# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah.
"""

# Menghapus data null
df = df.dropna()

"""dropna() digunakan untuk menghapus setiap data null yang ada di setiap kolomnya"""

df

df.isna().sum()

# Menghapus data duplikat
df = df.drop_duplicates()

df.duplicated().sum()

"""Sekarang, dataset tidak memiliki data duplikat lagi, dan dikarenakan data yang duplikat sedikit, maka saya drop saja karena tidak akan terlalu berdampak pada pengurangan informasi."""

df.info()

"""df.info() saya gunakan untuk mengetahui kondisi saat ini setelah saya melakukan pembersihan data null dan duplikat."""

# Mendeteksi outlier dengan metode IQR
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

for col in valid_num_cols:
    outliers, low, up = detect_outliers_iqr(df, col)
    print(f"Kolom: {col}")
    print(f"Jumlah outlier: {len(outliers)}")
    print(outliers[[col]].head())
    print("-" * 40)

plt.figure(figsize=(18, 12))
for i, col in enumerate(valid_num_cols[:9]):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(data=df, y=col, color='skyblue')
    plt.title(f'Boxplot - {col}')
    plt.tight_layout()

plt.suptitle('Visualisasi Outlier Kolom Numerik', fontsize=16, y=1.02)
plt.show()

"""Outlier ini kita simpan, karena ini adalah data yang diambil dari dunia nyata, dan data yang outlier disini masih dalam jangka yang normal (tidak melewati batas normal, seperti umur 200 tahun atau sebagainya)"""

df

#Melihat kembali persebaran data setelah preprocessing
df.describe()

"""Dilihat dari persebaran data numerik, tidak ada yang melewati batas normal. Contohnya, umur yang paling tinggi adalah 61. Kemudian, dari persebaran datanya, setiap kolom memiliki persebaran yang normal."""

# Urutan label target sesuai yang diinginkan
order = [
    'Insufficient_Weight',
    'Normal_Weight',
    'Overweight_Level_I',
    'Overweight_Level_II',
    'Obesity_Type_I',
    'Obesity_Type_II',
    'Obesity_Type_III'
]
# Buat salinan data
data = df.copy()

# Mapping target ke angka
target_map = {label: idx for idx, label in enumerate(order)}
data['NObeyesdad'] = data['NObeyesdad'].map(target_map)

# Label Encoding fitur kategorikal selain target
label_encoders = {}
for col in data.select_dtypes(include='object').columns:
    if col != 'NObeyesdad':
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le

# Normalisasi untuk kolom numerik (selain target)
scaler = StandardScaler()
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.drop('NObeyesdad')
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

"""Menetapkan map manual urutan indeks sebelum dilakukannya normalisasi kolom target bertujuan untuk menjaga konsistensi urutan data setelah dilakuaknnya label encoder. Nantinya, kolom target akan bernilai 0-6, Karena menggunakan data ordinal, setiap kenaikan angka haruslah berindikasi pada kenaikan level obesitas."""

# Melakukan pemisahan data, yaitu 80% train dan 20% test
X = data.drop('NObeyesdad', axis=1)
y = data['NObeyesdad']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("Distribusi label setelah SMOTE:\n", pd.Series(y_train_sm).value_counts())

# Hitung distribusi label hasil SMOTE dan urutkan
label_counts = pd.Series(y_train_sm).value_counts().sort_index()

# Ambil nama-nama label aslinya berdasarkan index
label_names = [order[i] for i in label_counts.index]

# Visualisasi
plt.figure(figsize=(10, 6))
sns.barplot(x=label_names, y=label_counts.values, palette='Set2')
plt.title('Distribusi Kelas Target (Setelah SMOTE)', fontsize=16)
plt.xlabel('Kategori Obesitas')
plt.ylabel('Jumlah Sampel')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Bisa dilihat, bahwa setiap kolomnya sudah menjadi data yang balanced (setelah dilakukannya SMOTE)

# **6. Pembangunan Model Clustering**

## **a. Pembangunan Model Clustering**

Pada tahap ini, Saya membangun model klasifikasi dengan memilih algoritma yang sesuai untuk mengklasifikasi data berdasarkan kolom target. Berikut adalah tahapannya.
1. Membuat 2 model, yaitu  SVM dan Random Forest dengan 3 perlakuan:

  *   Membangun model sebelum Hyperparameter Tuning & sebelum Feature Selection
  *   Membangun model sebelum Hyperparameter Tuning & sesudah Feature Selection
  *   Membangun model sesudah Hyperparameter Tuning & sesudah Feature Selection

### Pembangunan Model Sebelum Hyperparameter Tuning & Sebelum Feature Selection
"""

# SVM tanpa hyperparameter tuning
print("Training SVM tanpa hyperparameter tuning...")
svm = SVC()
svm.fit(X_train_sm, y_train_sm)
y_pred_svm = svm.predict(X_test)
print("Akurasi SVM:", accuracy_score(y_test, y_pred_svm))
joblib.dump(svm, 'svm_sebelum_tuning_FS_tuning.pkl')

# Random Forest tanpa hyperparameter tuning
print("Training Random Forest tanpa hyperparameter tuning...")
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_sm, y_train_sm)
y_pred_rf = rf.predict(X_test)
print("Akurasi Random Forest:", accuracy_score(y_test, y_pred_rf))
joblib.dump(rf, 'rf_sebelum_tuning_FS_tuning.pkl')

# Simpan fitur untuk dokumentasi
joblib.dump(X_train.columns.tolist(), 'feature_columns_before_fs_hyper_tuning.pkl')

"""Hasil Training didapatkan bahwa SVM dan Random Forest memiliki nilai yang cukup baik, yaitu lebih dari 85%, namun untuk SVM seharusnya bisa kita tingkatkan.

### Pembangunan Model Sesudah Hyperparameter & Tuning Sebelum Feature Selection
"""

# Support Vector Machine (SVM) with Hyperparameter Tuning
print("Tuning SVM...")
svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

svm_grid = GridSearchCV(SVC(), svm_params, cv=5, scoring='accuracy', n_jobs=-1)
svm_grid.fit(X_train_sm, y_train_sm)

best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test)

print("\nSVM (Best Params):", svm_grid.best_params_)
print("Akurasi:", accuracy_score(y_test, y_pred_svm))

# Random Forest with Hyperparameter Tuning
print("Tuning Random Forest...")
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid.fit(X_train_sm, y_train_sm)

best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test)

print("\nRandom Forest (Best Params):", rf_grid.best_params_)
print("Akurasi:", accuracy_score(y_test, y_pred_rf))

# Simpan model sebelum feature selection
joblib.dump(best_svm, 'svm_sebelum_feature_selection.pkl')
joblib.dump(best_rf, 'rf_sebelum_feature_selection.pkl')
joblib.dump(X_train.columns.tolist(), 'feature_columns_before_fs.pkl')

"""Setelah dilakukan hyperparameter tuning, bisa dilihat bahwa akurasi SVM meningkat jauh, dengan mencapai 96%, namun akuasi dari Random Forest sedikit menurun, tetapi masih dalam akurasi yang bagus.

## **b. Feature Selection**
"""

# Pakai Random Forest untuk estimasi pentingnya fiturfsfefs
model = RandomForestClassifier(random_state=42)

# RFE untuk memilih 10 fitur terbaik
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(X, y)

# Lihat fitur penting
selected_features = X.columns[rfe.support_]
print("Fitur Terpilih oleh RFE:")
print(selected_features)

ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': rfe.ranking_})
ranking = ranking.sort_values('Ranking')
print(ranking)

"""Yang diambil hanyalah 10 feature dengan rank 1, atau yang paling baik dari hasil sorting berdasarkan RFE.

### Pembangunan Model Sesudah Feature Selection dan Hyperparameter Tuning
"""

# Subset fitur
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# SMOTE
smote = SMOTE(random_state=42)
X_train_selected_sm, y_train_sm = smote.fit_resample(X_train_selected, y_train)

# Training SVM dan Random Forest dengan grid search sudah siap
svm_grid.fit(X_train_selected_sm, y_train_sm)
best_svm_selected = svm_grid.best_estimator_

rf_grid.fit(X_train_selected_sm, y_train_sm)
best_rf_selected = rf_grid.best_estimator_

# Prediksi dan evaluasi
y_pred_svm_selected = best_svm_selected.predict(X_test_selected)
y_pred_rf_selected = best_rf_selected.predict(X_test_selected)

print("\nSVM (Best Params) - Setelah Feature Selection:", svm_grid.best_params_)
print("Akurasi SVM:", accuracy_score(y_test, y_pred_svm_selected))

print("\nRandom Forest (Best Params) - Setelah Feature Selection:", rf_grid.best_params_)
print("Akurasi RF:", accuracy_score(y_test, y_pred_rf_selected))

# Simpan model dan fitur
joblib.dump(best_svm_selected, 'svm_setelah_feature_selection.pkl')
joblib.dump(best_rf_selected, 'rf_setelah_feature_selection.pkl')
joblib.dump(selected_features, 'feature_columns_after_fs.pkl')

"""Setelah Feature selection dan hyperparameter tuning, akurasi SVM naik sedikit, menjadikannya model terbaik sejauh ini, sedangkan Random Forest semakin menurun, tetapi masih dalam akurasi yang sangat baik.

## **c. Evaluasi Model Clustering**

Untuk menentukan jumlah cluster yang optimal dalam model clustering, Anda dapat menggunakan metode Elbow atau Silhouette Score.

Metode ini membantu kita menemukan jumlah cluster yang memberikan pemisahan terbaik antar kelompok data, sehingga model yang dibangun dapat lebih efektif. Berikut adalah **rekomendasi** tahapannya.
1. Gunakan Silhouette Score dan Elbow Method untuk menentukan jumlah cluster optimal.
2. Hitung Silhouette Score sebagai ukuran kualitas cluster.

Kita dapatkan bahwa untuk model DBSCAN ini eps paling baik adalah di bawah sekitar di bawah 0.6, yang hampir mencapai nilai silhouette score sebesar 0.6
"""

# Menentukan Evaluasi apa saja yang akan dilakukan
def evaluate_model(model, X_test, y_test, model_name="Model"):
    y_pred = model.predict(X_test)

    # Hitung metrik
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision (Macro)": precision_score(y_test, y_pred, average='macro'),
        "Precision (Weighted)": precision_score(y_test, y_pred, average='weighted'),
        "Recall (Macro)": recall_score(y_test, y_pred, average='macro'),
        "Recall (Weighted)": recall_score(y_test, y_pred, average='weighted'),
        "F1 Score (Macro)": f1_score(y_test, y_pred, average='macro'),
        "F1 Score (Weighted)": f1_score(y_test, y_pred, average='weighted'),
        "Cohen’s Kappa": cohen_kappa_score(y_test, y_pred),
        "Matthews Corrcoef": matthews_corrcoef(y_test, y_pred)
    }

    print(f"\n Evaluation for {model_name}")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

    # Confusion matrix
    plt.figure(figsize=(6, 5))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu')
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.tight_layout()
    plt.show()

"""Disini saya menggunakan 9 metrik evaluasi agar memberikan informasi yang lebih komprehensif, serta disini saya akan mengevaluasi 6 model (2 model dengan 3 kali perlakuan) secara terpisah.

### Sebelum Hyperparameter Tuning dan Feature Selection

#### SVM
"""

# SVM Sebelum Feature Selection
model_svm_before = joblib.load('svm_sebelum_tuning_FS_tuning.pkl')
cols_svm_before = joblib.load('feature_columns_before_fs.pkl')
X_test_svm_before = X_test.reindex(columns=cols_svm_before, fill_value=0)
evaluate_model(model_svm_before, X_test_svm_before, y_test, model_name="SVM Sebelum Tuning dan Feature Selection")

"""Model ini memiliki performa yang baik dengan akurasi 86,12% dan F1-score makro 85,73%, menunjukkan prediksi yang cukup seimbang di semua kelas. Kelas 0, 5, dan 6 tampil sangat baik, sementara kelas 1 dan 3 masih bisa ditingkatkan. Nilai Cohen’s Kappa dan Matthews Correlation Coefficient di atas 0,83 menandakan prediksi yang konsisten dan jauh dari kebetulan.

#### Random Forest
"""

# Random Forest Sebelum Feature Selection
model_rf_before = joblib.load('rf_sebelum_tuning_FS_tuning.pkl')
cols_rf_before = joblib.load('feature_columns_before_fs.pkl')
X_test_rf_before = X_test.reindex(columns=cols_rf_before, fill_value=0)
evaluate_model(model_rf_before, X_test_rf_before, y_test, model_name="Random Forest Sebelum Tuning dan Feature Selection")

"""Model ini menunjukkan performa sangat tinggi dengan akurasi 95,45% dan F1-score makro 95,32%, mencerminkan keseimbangan yang sangat baik antar kelas. Kelas 0 hingga 6 memiliki presisi dan recall tinggi, terutama kelas 4 dan 6 yang nyaris sempurna. Nilai Cohen’s Kappa dan Matthews Correlation Coefficient mendekati 0,95, menandakan kualitas prediksi yang sangat kuat dan andal.

### Sesudah Hyperparameter Tuning & Sebelum Feature Selection

#### SVM
"""

# SVM Sebelum Feature Selection dan sesudah Hyperparameter Tuning
model_svm_before = joblib.load('svm_sebelum_feature_selection.pkl')
cols_svm_before = joblib.load('feature_columns_before_fs.pkl')
X_test_svm_before = X_test.reindex(columns=cols_svm_before, fill_value=0)
evaluate_model(model_svm_before, X_test_svm_before, y_test, model_name="SVM Sebelum Feature Selection & Setelah Tuning")

"""Model ini mencapai akurasi sangat tinggi sebesar 96,17% dengan F1-score makro 96,11%, menunjukkan prediksi yang sangat konsisten dan akurat di hampir semua kelas. Setiap kelas memiliki performa kuat, dengan presisi dan recall tinggi, terutama kelas 0, 4, dan 6 yang mendekati sempurna. Nilai Cohen’s Kappa dan Matthews Correlation Coefficient di atas 0,95 memperkuat bahwa model ini sangat andal dan minim kesalahan acak.

#### Random Forest
"""

# Random Forest Sebelum Feature Selection dan sesudah Hyperparameter Tuning
model_rf_before = joblib.load('rf_sebelum_feature_selection.pkl')
cols_rf_before = joblib.load('feature_columns_before_fs.pkl')
X_test_rf_before = X_test.reindex(columns=cols_rf_before, fill_value=0)
evaluate_model(model_rf_before, X_test_rf_before, y_test, model_name="Random Forest Sebelum Feature Selection dan Setelah Tuning")

"""Model ini memiliki akurasi tinggi sebesar 95,22% dengan F1-score makro 95,07%, menunjukkan performa yang sangat baik secara keseluruhan. Hampir semua kelas menunjukkan presisi dan recall tinggi, terutama kelas 0, 4, 5, dan 6. Kelas 1 sedikit lebih rendah performanya dibanding yang lain, namun tetap solid. Nilai Cohen’s Kappa dan Matthews Corrcoef sekitar 0,944 mengindikasikan prediksi yang sangat andal dan stabil.

### Sesudah Feature Selection & Sesudah Hyperparameter Tuning

#### SVM
"""

# SVM Setelah Feature Selection dan Hyperparameter Tuning
model_svm_after = joblib.load('svm_setelah_feature_selection.pkl')
cols_svm_after = joblib.load('feature_columns_after_fs.pkl')
X_test_svm_after = X_test.reindex(columns=cols_svm_after, fill_value=0)
evaluate_model(model_svm_after, X_test_svm_after, y_test, model_name="SVM Setelah Feature Selection & Setelah Tuning")

"""Setelah feature selection, model SVM menunjukkan performa sangat tinggi dengan akurasi 96,41% dan F1-score makro 96,25%. Semua kelas tampil sangat baik, terutama kelas 0, 4, 5, dan 6 yang hampir sempurna. Kelas 2 sedikit lebih rendah recall-nya, namun masih sangat kompetitif. Nilai Cohen’s Kappa dan Matthews Corrcoef di atas 0,95 menunjukkan bahwa model sangat stabil dan prediksinya sangat dapat diandalkan.

#### Random Forest
"""

# Random Forest Setelah Feature Selection dan Hyperparameter Tuning
model_rf_after = joblib.load('rf_setelah_feature_selection.pkl')
cols_rf_after = joblib.load('feature_columns_after_fs.pkl')
X_test_rf_after = X_test.reindex(columns=cols_rf_after, fill_value=0)
evaluate_model(model_rf_after, X_test_rf_after, y_test, model_name="Random Forest Setelah Feature Selection")

"""Model ini mencapai akurasi sebesar 94,02% dengan performa yang solid secara keseluruhan, meskipun sedikit lebih rendah dibanding model sebelumnya. F1-score makro sebesar 93,82% menunjukkan keseimbangan yang baik antar kelas, meski recall pada kelas 2 dan 3 masih bisa ditingkatkan. Sementara itu, nilai Cohen’s Kappa dan Matthews Corrcoef di atas 0,93 menandakan prediksi yang sangat konsisten dan dapat diandalkan.

# **7. Conclusion**

### **1. Sebelum Hyperparameter Tuning & Feature Selection**

Pada tahap awal, model **Support Vector Machine (SVM)** mencatatkan akurasi sebesar **86.12%**, dengan macro F1-score **0.8573** dan weighted F1-score **0.8613**. Meskipun tergolong baik, performa masih kurang optimal terutama pada kelas-kelas minor seperti kelas 1 dan kelas 2, yang hanya mencapai F1-score sekitar **0.73–0.75**. Hal ini mengindikasikan adanya ketidakseimbangan performa antar kelas.

Nilai **Cohen’s Kappa** dan **Matthews Correlation Coefficient (MCC)** berada di angka **0.8379** dan **0.8382**, menunjukkan model sudah cukup stabil, namun masih terdapat ruang perbaikan dalam hal akurasi prediksi lintas kelas.

Sebaliknya, model **Random Forest** sudah menunjukkan performa awal yang sangat baik, dengan akurasi **95.45%**, serta F1-score makro dan tertimbang di atas **0.95**. Ini membuktikan bahwa meskipun tanpa tuning, Random Forest sudah sangat kompetitif dalam menangani klasifikasi awal.

---

### **2. Setelah Hyperparameter Tuning (Tanpa Feature Selection)**

Setelah dilakukan hyperparameter tuning, performa **SVM** meningkat signifikan. Akurasi melonjak ke **96.17%**, dengan macro F1-score **0.9611** dan weighted F1-score **0.9617**. Peningkatan tajam juga terlihat pada kelas-kelas minor, di mana precision dan recall meningkat secara konsisten.

Nilai **Cohen’s Kappa** dan **MCC** masing-masing menjadi **0.9553** dan **0.9554**, memperkuat bahwa tuning berhasil memperbaiki kestabilan dan kemampuan generalisasi model. Model menjadi lebih akurat tanpa mengorbankan keseimbangan antar kelas.

Model **Random Forest** setelah tuning justru mengalami sedikit penurunan performa (akurasi turun menjadi **95.22%**, F1-score weighted **0.9525**). Ini menunjukkan bahwa tuning tidak selalu memberikan dampak positif jika tidak dilakukan secara optimal, atau bisa jadi model awalnya sudah berada pada konfigurasi yang hampir ideal.

---

### **3. Setelah Hyperparameter Tuning & Feature Selection**

Tahap akhir yang menggabungkan hyperparameter tuning dan **feature selection** memberikan hasil terbaik. Model **SVM** berhasil mencapai akurasi **96.41%**, macro F1-score **0.9625**, dan weighted F1-score **0.9639**. Hampir seluruh kelas, termasuk kelas minor seperti kelas 1 dan 2, memiliki F1-score mendekati atau melebihi **0.97**.

Nilai **Cohen’s Kappa** dan **MCC** juga meningkat menjadi **0.9581** dan **0.9582**, menandakan konsistensi yang sangat tinggi dalam prediksi lintas kelas. Feature selection terbukti sangat efektif dalam menyaring fitur-fitur yang relevan, meningkatkan efisiensi model, serta mengurangi potensi overfitting.

Model **Random Forest** juga mengalami peningkatan kembali setelah feature selection (akurasi menjadi **95.74%**, weighted F1-score **0.9570**), meskipun tetap sedikit di bawah performa SVM. Namun, Random Forest tetap unggul dalam aspek interpretabilitas dan stabilitas prediksi di lingkungan yang lebih kompleks.

---

### **Kesimpulan Akhir**

Berdasarkan seluruh proses evaluasi, dapat disimpulkan bahwa **kombinasi Support Vector Machine dengan hyperparameter tuning dan feature selection memberikan hasil klasifikasi terbaik**. Model ini tidak hanya unggul dalam akurasi (**96.41%**), tetapi juga menunjukkan performa seimbang dan stabil di seluruh kelas target. Oleh karena itu, **SVM merupakan pilihan model final yang direkomendasikan** untuk implementasi klasifikasi **tingkat obesitas** berdasarkan data pengguna, dengan tingkat keandalan prediksi yang sangat tinggi.
"""